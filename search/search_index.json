{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BrandonLin's Personal Blog","text":"<p>Hi, I'm Brandon Lin, an undergraduate student now studying at Shanghai Jiao Tong University.</p> <p>My github link: Click here!</p>"},{"location":"Cookbooks/Espanso/","title":"Snippets based on Espanso","text":"<ul> <li>Official site of Espanso: Click here!</li> </ul> <p>Espanso detects when you type a keyword and replaces it while you're typing.</p>"},{"location":"Cookbooks/Espanso/#configuration","title":"Configuration","text":"<ul> <li>French Typing based on a American keyboard: French-accents By Otto Piramuthu or visit github. Install by command:     <pre><code>espanso install french-accents\n</code></pre></li> </ul>"},{"location":"Cookbooks/Mkdocs/","title":"Mkdocs","text":""},{"location":"Cookbooks/Mkdocs/#installation","title":"Installation","text":"<pre><code>pip install mkdocs\npip install mkdocs-material\npip install https://github.com/mitya57/python-markdown-math/archive/master.zip`\n</code></pre>"},{"location":"Cookbooks/Mkdocs/#create-server-and-publishment","title":"Create, Server and Publishment","text":"<ul> <li> <p>Create a projet: <code>mkdocs new [filename]</code></p> </li> <li> <p>Render to a site: <code>mkdocs serve</code></p> </li> <li> <p>Publish: <code>mkdocs gh-deploy</code></p> </li> </ul>"},{"location":"Cookbooks/Mkdocs/#code","title":"Code","text":"<ul> <li>Code Naming, Code Highlighting, Line Numeration: By adding <code>title=\"name\" linenums=\"1\" hl_lines=\"index\"</code> at the right of the first ```, example : </li> </ul> first.py<pre><code>print(\"Hello World\")\n# Yes, it works!\n</code></pre>"},{"location":"Cookbooks/Mkdocs/#template-for-mkdocsyml","title":"Template for mkdocs.yml","text":"<pre><code>site_name: Test # Change here\nnav:\n- Home: 'index.md'\n- 'Cookbooks': - 'Mkdocs': 'Cookbooks/Mkdocs.md'\ntheme: name: material\nfeatures:\n- naviagation.tabs\n- naviagation.sections\n- toc.integrate\n- navigation.top\n- search.suggest\n- search.highlight\n- content.tabs.link\n- content.code.annotation\n- content.code.copy\nlanguage: en\npalette:\n- scheme: default\ntoggle:\nicon: material/toggle-switch-off-outline\nname: Switch to dark mode\nprimary: teal\naccent: purple\n- scheme: slate\ntoggle:\nicon: material/toggle-switch\nname: Switch to light mode\nprimary: teal\naccent: lime\nextra:\nsocial:\n- icon: fontawesome/brands/github-alt\nlink: https://github.com/Languisher # Change to your github link\ncopyright: | # Change based on your own personal information\n&amp;copy; 2023 &lt;a href=\"https://github.com/Languisher\"  target=\"_blank\" rel=\"noopener\"&gt;Brandon Lin&lt;/a&gt;\nextra_javascript: - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\nmarkdown_extensions:\n- pymdownx.highlight:\nanchor_linenums: true\n- pymdownx.inlinehilite\n- pymdownx.snippets\n- admonition\n- pymdownx.arithmatex:\ngeneric: true\n- footnotes\n- pymdownx.details\n- pymdownx.superfences\n- pymdownx.mark\n- attr_list\n- mdx_math\n</code></pre> <p>Explications:</p> <ul> <li>The <code>nav</code> configuration setting in your mkdocs.yml file defines which pages are included in the global site navigation menu as well as the structure of that menu. If not provided, the navigation will be automatically created by discovering all the Markdown files in the documentation directory.</li> </ul>"},{"location":"Cookbooks/Mkdocs/#references","title":"References","text":"<ul> <li>Basic Tutorial: https://www.mkdocs.org/user-guide/</li> <li> <p>Material Tutorial: https://www.youtube.com/watch?v=Q-YA_dA8C20</p> </li> <li> <p>Mathjax problem: https://stackoverflow.com/questions/27882261/mkdocs-and-mathjax</p> </li> </ul>"},{"location":"Cookbooks/Vim/","title":"Vim","text":""},{"location":"M/AL/Endomorphismes/","title":"Endomorphismes","text":"<p>Exercices : TD</p> <p>Notations :</p> <ul> <li> <p>\\(E\\) est un espace vectoriel euclidien de dimension \\(n\\).</p> </li> <li> <p>\\(\\mathscr E = (e_1,\\cdots, e_n)\\), \\(\\mathscr B=(b_1,\\cdots, b_n)\\) deux bases orthonorm\u00e9es de \\(E\\).</p> </li> <li> <p>\\(P_{\\mathscr B}^{\\mathscr E}\\) le matrice de passage d'une matrice muni d'une base \\(\\mathscr E\\) dans celle d'une base \\(\\mathscr B\\).</p> </li> <li> <p>\\(\\mathscr F=(f_1,\\cdots,f_n)\\) une base quelconque de \\(E\\).</p> </li> <li> <p>\\(\\phi:(x,y) \\mapsto &lt;x,y&gt;\\) le produit scalaire de \\(E\\)</p> </li> <li> <p>\\(\\Phi=\\mathrm{Mat}_\\mathscr{F}(\\phi)= [\\phi(f_i,f_j)]_{(i,j)\\in [\\![1,n]\\!]}\\in \\mathrm{S}_n(\\mathbb R)\\) la matrice de \\(\\phi\\) dans la base \\(\\mathscr F\\).</p> </li> </ul> <p>D\u00e9monstration :   </p>"},{"location":"M/AL/Endomorphismes/#resume","title":"R\u00e9sum\u00e9","text":"<ul> <li> \\[   x\\in E \\iff \\begin{cases}\\mathrm{Mat}_{\\mathscr E}(x) = [&lt;e_i,x&gt;]_i\\in \\mathrm{M}_{m,1}\\\\\\mathrm{Mat}_{\\mathscr{B}}(x) = \\;^tP_{\\mathscr E}^\\mathscr B\\cdot \\mathrm{Mat}_\\mathscr E(x)\\end{cases}   \\] </li> <li> \\[   u \\in \\mathscr{L}(E) \\iff \\begin{cases}\\mathrm{Mat}_\\mathscr E(u) = [&lt;e_i,u(e_j)&gt;]_{(i,j)} \\\\ \\mathrm{Mat}_\\mathscr B(u) = \\;^tP_\\mathscr{E}^{\\mathscr{B}}.\\mathrm{Mat}_\\mathscr{E}(u).P_\\mathscr E^\\mathscr B\\\\ \\boxed{\\mathrm{Mat}_\\mathscr E(u^\\star)=\\;^t\\mathrm{Mat}_\\mathscr E(u)}\\end{cases}   \\] </li> <li> \\[   u \\in \\mathscr S(E) \\iff \\begin{cases} \\;^t\\mathrm{Mat}_\\mathscr E(u) = \\mathrm{Mat}_\\mathscr E(u)\\\\\\mathrm{Mat}_\\mathscr E(u) \\in \\mathrm{S}_n(\\mathbb R)\\end{cases}   \\] </li> <li> \\[   u \\in \\mathscr O(E) \\iff \\begin{cases} \\;^t\\mathrm{Mat}_\\mathscr E(u) =( \\mathrm{Mat}_\\mathscr E(u))^{-1}\\\\\\mathrm{Mat}_\\mathscr E(u) \\in \\mathrm{O}_n(\\mathbb R)\\end{cases}   \\] </li> <li> \\[   u \\in \\mathscr A(E) \\iff \\begin{cases} \\;^t\\mathrm{Mat}_\\mathscr E(u) =- \\mathrm{Mat}_\\mathscr E(u)\\\\\\mathrm{Mat}_\\mathscr E(u) \\in \\mathrm{A}_n(\\mathbb R)\\end{cases}   \\] </li> <li> <p>Si on a le choix de la base dans un espace euclidien, on choisit prequement toujours de travailler en base orthonorm\u00e9e !</p> </li> </ul>"},{"location":"M/AL/Endomorphismes/#endomorphismes-auto-ajoints","title":"Endomorphismes auto-ajoints","text":"<ul> <li> <p>\\(f\\) est auto-adjoint ou sym\u00e9trique si      </p> </li> <li> <p>On note          en m\u00eame temps,       </p> </li> <li> <p>Exemples :</p> <ul> <li> <p>Les projecteurs orthogonaux sont les projecteurs auto-adjoints.</p> </li> <li> <p>Si \\(\\exists \\; \\mathscr E, \\; \\exists \\Lambda = (\\lambda_1,\\cdots,\\lambda_n) \\in \\mathbb R^n\\), tel que :  Alors, \\(f\\) est auto-adjoint.</p> </li> </ul> </li> <li> <p>\\(f\\) est un endomorphismes auto-adjoint positif s'il v\u00e9rfie de plus           Version Matricielle :       </p> </li> <li> <p>\\(f\\) est un endomorphismes auto-adjoint d\u00e9finis positif s'il v\u00e9rfie de plus           Version Matricielle :      </p> </li> <li> <p>Propri\u00e9t\u00e9s : </p> <ul> <li> <p>Si \\(n = \\dim E\\),  </p> </li> <li> <p>Pour n'import quelle \\(f\\),  De plus,   </p> </li> </ul> <p>Preuve :       </p> <ul> <li>Pour n'import quelle \\(f\\),  </li> </ul> </li> </ul>"},{"location":"M/AL/Endomorphismes/#theoreme-de-reduction-des-endomorphismes-auto-adjointes","title":"Th\u00e9or\u00e8me de r\u00e9duction des endomorphismes auto-adjointes","text":"\\[ f\\in \\mathscr S(E) \\implies \\exists (\\mathscr E, \\Lambda) \\in (E ^ n, \\mathbb R^n),\\; \\forall k\\in [\\![1,n]\\!],\\; f(e_k)=\\lambda_k.e_k $$ - Verison Matricielle :     $$     f\\in \\mathscr S(E) \\implies \\exists \\mathscr E \\in E ^ n,\\;  \\mathrm {Mat}(f,\\mathscr E) = \\mathrm{diag}(\\lambda_1,\\cdots,\\lambda_n)     \\] <pre><code>&gt; [Preuve](assets/Theoreme-de-reduction.png)\n</code></pre> <ul> <li> <p>Remarque :      </p> <p>Preuve</p> </li> <li> <p>Remarque : </p> </li> <li> <p>Proposition : Soit \\(f\\in \\mathscr S^ +(E)\\), alors :      </p> <p>Preuve</p> </li> </ul>"},{"location":"M/AL/Endomorphismes/#automorphismes-orthogonaux","title":"Automorphismes Orthogonaux","text":"<ul> <li> <p>\\(f \\in \\mathscr{GL}(E)\\) est appell\u00e9e automorphisme orthogonal si elle v\u00e9rifie :  </p> </li> <li> <p>\u00c9videmment,       </p> </li> <li> <p>Propri\u00e9t\u00e9s : \u00c9quivalance de</p> <ul> <li>\\(f\\in \\mathscr O(E)\\)</li> <li>\\(\\forall x\\in E, \\;||f(x)||=||x||\\)</li> <li>\\(\\forall(x,y)\\in E ^ 2,\\; &lt;f(x),f(y) = &lt;x,y&gt;\\)</li> <li>\\(\\forall, \\exists\\) \\(\\mathscr E\\) t.q. \\(f(e_1),\\cdots,f(e_n)\\) encore une b.o.n.</li> </ul> </li> <li> <p>On appelle sym\u00e9trie orthogonale tout \u00e9l\u00e9ment de       </p> <ul> <li>Son projecteur associ\u00e9 est \\(s = 2p-\\mathrm{id}_E\\) est un projecteur orthogonale.</li> </ul> </li> <li> <p>On appelle rotation de \\(E\\) :      </p> </li> <li> <p>On appelle r\u00e9flexion de \\(E\\) :      </p> <ul> <li>C'est une sym\u00e9trie orthogonale de \\(E_1\\), l'hyperplan sur lequelle une projection orthogonale. \\(E_1\\) est d\u00e9finie par </li> </ul> </li> <li> <p>Exemple de dimension 2 : </p> </li> <li> <p>Exemple de dimension 3 : </p> </li> </ul>"},{"location":"M/AL/Endomorphismes/#decomposition-dun-automorphisme-orthogonaux-en-un-produit-de-reflexions","title":"D\u00e9composition d'un automorphisme orthogonaux en un produit de r\u00e9flexions","text":"<ul> <li> <p>Soit \\(f\\in \\mathscr{O}(E)\\),      </p> <p>Preuve</p> </li> <li> <p>Forme Canonique des automorphismes orthogonaux : </p> </li> </ul>"},{"location":"M/AL/Endomorphismes/#endomorphismes-antisymetriques","title":"Endomorphismes Antisym\u00e9triques","text":"<ul> <li> <p>On dit que \\(f\\) est anti-sym\u00e9trique s'il v\u00e9rifie       </p> </li> <li> <p>Propri\u00e9t\u00e9s :</p> <ul> <li>D\u00e9composition en somme directe  </li> <li>Dimension :  </li> </ul> </li> <li> <p>Propri\u00e9t\u00e9 :      </p> </li> <li> <p>Forme Canonique d'un endomorphismes anti-sym\u00e9trique : </p> <p>Preuve</p> </li> <li> <p>Propri\u00e9t\u00e9s : Dans \\(E= \\mathbb R^ 3\\)      et on peut trouver une b.o.n. telle que           Sa forme canonique bas\u00e9e sur \\((c_1,c_2,c_3)\\) est           associ\u00e9 \u00e0 \\(w= pc_1+qc_2+rc_3\\).</p> </li> </ul>"},{"location":"M/AL/Endomorphismes/#decompositions","title":"D\u00e9compositions","text":""},{"location":"M/AL/Endomorphismes/#decomposition-polaire","title":"D\u00e9composition Polaire","text":"<ul> <li> <p>Soit \\(f \\in \\mathscr{GL}(E)\\),       </p> <p>Preuve</p> </li> </ul>"},{"location":"M/AL/Endomorphismes/#decomposition-de-cholesky","title":"D\u00e9composition de Cholesky","text":"<ul> <li> <p>Si \\(A \\in \\mathrm{S}_n^{++}(\\mathbb R)\\), il existe une unique matrice \\(T\\in \\mathrm{T}_n^+ \\cap \\mathrm{GL}_n(\\mathbb R)\\) \u00e0 coefficients diagonaux \\(&gt;0\\) tel que :     </p> </li> <li> <p>En ce cas, \\(AX = Y \\iff \\begin{cases} \\;^tT.Z = Y\\\\ T.X = Z\\end{cases}\\)</p> </li> </ul> <p>D\u00e9monstration :</p> <ul> <li> <p>Analyse : L'orthomalisation de Gram-Schimidt appliqu\u00e9 \u00e0 \\(\\mathscr{B}=(b_1, \\cdots, b_n)\\) donne \\(\\mathscr{E} = (e_1,\\cdots, e_n)\\). On trouvera       On sait que \\(u \\in \\mathscr{S}^{++}\\), donc \\(\\exists ! v\\in \\mathscr{S}^{++}\\) tel que \\(u=v \\circ v\\), en m\u00eame temps \\(&lt;c_i,u(c_j) = &lt;v(c_i),v(c_j)&gt;\\)</p> </li> <li> <p>Synth\u00e8se :</p> </li> <li> <p>Existence : \\(T = [&lt;e_i,b_j&gt;]\\), o\u00f9 \\((e_1,\\cdots,e_n)\\) orthonorm\u00e9e de \\((b_1, \\cdots,b_n)\\) bien suffit.</p> </li> <li>Unicit\u00e9 : Si \\(A = \\;^tT_1.T_1 = \\;^tT_2.T_2\\), donc \\(\\Delta=T_1.T_2^{-1} = \\;^tT_1^{-1}.^tT_2\\), appartient respectivement \u00e0 \\(\\mathrm{T}^+\\) et \\(\\mathrm{T}^-\\). Dsonc \\(\\Delta\\) est diagonale. Or, \\(\\(A = \\;^tT_1.T_1 = \\;^tT_2.(^t\\Delta.\\Delta).T_2=\\;^tT_2.T_2\\)\\). Cela implique que \\(\\Delta = \\mathrm{id}_E\\).</li> </ul>"},{"location":"M/AL/Endomorphismes/#decomposition-qr","title":"D\u00e9composition QR","text":"<ul> <li> <p>Soit \\(A \\in \\mathrm{GL}_n(\\mathbb R)\\), il existe une unique \\((O,T)\\in (\\mathrm O_n(\\mathbb R),\\mathrm T_n^+(\\mathbb{R}))\\), tel que :      </p> <p>Preuve</p> </li> <li> <p>Si on veut r\u00e9soudre \\(AX = B\\), cela devient \\(O.T.X=B\\), ensuite          car \\(^tO=O^{-1}\\)</p> </li> </ul>"},{"location":"M/AL/Endomorphismes/#matrice-de-gram","title":"Matrice de Gram","text":"<ul> <li> <p>Soit \\(E\\) un espace pr\u00e9hilbertien r\u00e9el, et \\(\\mathscr X=(x_1,\\cdots,x_n)\\) une famille finie de vecteurs de \\(E\\). On appelle matrice de Gram de \\(\\mathscr X\\) la matrice       </p> </li> <li> <p>\\(G_\\mathscr X \\in \\mathrm{S}_n^+(\\mathbb R)\\).</p> </li> </ul> <p>Preuve : Soit \\(X_a = \\;^t[a_1,\\cdots,a_n]\\) avec la base \\((e_1,\\cdots,e_n)\\) </p> <ul> <li> <p>\\(G_\\mathscr X \\in \\mathrm S_n^{++}(\\mathbb R) \\iff [(x_1,\\cdots,x_n) \\text { ind\u00e9pendants} ]\\)</p> <ul> <li>C'est \u00e9quivalent de dire \\(G_\\mathscr X\\) est de plus inversible.</li> </ul> </li> <li> <p>Plus pr\u00e9cis\u00e9ment,      </p> </li> <li> <p>Application : Pour montrer que \\((x_1,\\cdots,x_n)\\) ind\u00e9pendants ou non, il faut et il suffit de calculer \\(\\(\\det(G_\\mathscr X)\\)\\)</p> </li> <li> <p>Soit \\(E\\) pr\u00e9hilbertien r\u00e9el, \\(\\mathscr X = (x_1,\\cdots, x_n)\\in E^n\\), on appellera volume de l'objet construit sur \\((x_1,\\cdots,x_p)\\) :      </p> </li> </ul>"},{"location":"M/AL/Exercices-Endomorphismes/","title":"TD - Endomorphismes","text":""},{"location":"M/AL/Exercices-Endomorphismes/#td-8-e1","title":"TD 8 - E1","text":"<p>On consid\u00e8re      </p> <p>On pose pour \\((f,g)\\in E^2\\),      </p> <ol> <li> <p>Montrer rapidement que \\(E\\) est un espace vectoriel ? Que peut-on dire de sa dimension ?</p> </li> <li> <p>Montrer que \\(\\langle\\,,\\,\\rangle\\) est un produit scalaire sur \\(E\\).</p> </li> <li> <p>On consid\u00e8re les trois fonctions     </p> <p>Montrer que ces trois fonctions sont dans \\(E\\), qu'elles sont ind\u00e9pendantes et calculer le volume du parall\u00e9lipip\u00e8de construite \u00e0 partir de ces trois fonctions.</p> </li> </ol>"},{"location":"M/AL/Exercices-Endomorphismes/#correction","title":"Correction","text":"<ol> <li> <p>On va montrer que \\(E\\) est un sev de \\(\\mathscr C(\\mathbb R^+, \\mathbb R)\\)</p> <ul> <li> <p>La fonction nulle est dans \\(E\\), donc \\(E \\ne \\emptyset\\).</p> </li> <li> <p>Soit \\(\\lambda \\in \\mathbb R\\) et soit \\((f,g)\\in E\\), montrons que \\(\\lambda f+g\\in E\\). Soit \\(t\\in \\mathbb R^+\\).    </p> </li> <li> <p>NB : \\(f(t)g(t)e^{-3t}\\) est int\u00e9grable car \\(fg \\le (f+g)/2\\). </p> </li> <li> <p>Tous les parties sont int\u00e9grable, donc \\(\\lambda f+g \\in E\\).</p> </li> <li> <p>Donc \\(E\\) est un sev de \\(\\mathscr C(\\mathbb R^+, \\mathbb R)\\).</p> </li> </ul> </li> <li> <p>Montrons que \\(\\langle\\,,\\,\\rangle\\) est d\u00e9fini positif, sym\u00e9trique et bilin\u00e9aire.</p> <ul> <li>Soit \\(f\\in E\\), alors      </li> <li> <p>De plus, \\(t \\mapsto f^2(t)e^{-3t}\\) est positive et continue, donc \\(\\langle f,f \\rangle=0 \\implies f=0\\).</p> </li> <li> <p>\\(\\langle\\,f,g\\rangle = \\langle\\,g,f\\rangle\\)</p> </li> <li>\\(\\langle\\,f,\\lambda g+h\\rangle = \\lambda\\langle\\,f,g\\rangle +\\langle\\,f,h\\rangle\\)</li> </ul> </li> <li> <p>Calculerons          en utilisant <code>Python</code> :      <pre><code>def scal(f,g):\nt = sp.Symbol(\"t\")\nreturn sp.integrate(f(t)*g(t)*sp.exp(-3*t), (t, 0, +sp.oo))\nf = [sp.exp, sp.sin, sp.cos]\nGram = sp.zeros(3)\nfor i in range(3):\nfor j in range(3):\nGram[i,j] = scal(f[i], f[j])\nGram\n</code></pre>      Calculerons son d\u00e9terminant :       </p> </li> </ol>"},{"location":"M/AL/Exercices-Endomorphismes/#td8-e2","title":"TD8 - E2","text":"<p>Soit \\(E\\) un espace euclidien, \\(\\mathcal{E}\\) un espace affine de direction \\(E\\) et \\(\\phi\\) une application affine de \\(\\mathcal{E}\\) dans \\(\\mathcal{E}\\) dont l'application lin\u00e9aire sous-jacente sera not\u00e9e \\(\\overrightarrow{\\phi}\\).</p> <p>On pose  </p> <ol> <li> <p>Montrer que si \\(E_1=\\{0_{_E}\\}\\), alors \\(\\phi\\) poss\u00e8de un unique point fixe.</p> </li> <li> <p>On suppose que \\(E_1\\ne\\{0_{_E}\\}\\), montrer que \\(\\phi\\) est alors la compos\u00e9e d'une application affine ayant un point fixe et d'une translation (\u00e9ventuellement nulle).</p> </li> <li> <p>On suppose que   Montrer qu'on peut alors choisir la translation parall\u00e8lement \u00e0 \\(E_1\\).</p> </li> </ol>"},{"location":"M/An/Convexit%C3%A9/","title":"Convexit\u00e9","text":"<p>Exercices : TD3</p>"},{"location":"M/An/Convexit%C3%A9/#convexe-concave-affine","title":"Convexe, Concave, Affine","text":"<ul> <li> <p>D\u00e9finition : Soit \\(f:I\\to \\mathbb{R}\\), \\(f\\) est dite convexe si elle v\u00e9rifie       </p> </li> <li> <p>\\(f\\) est dite concave si elle v\u00e9rifie       </p> </li> <li> <p>Si \\(f:I\\to \\mathbb{R}\\) est \u00e0 la fois convexe et concave, alors \\(f\\) est affine.</p> </li> <li> <p>Propri\u00e9t\u00e9 : Sur l'intervalle \\([a,b]\\), la courbe passe en-dessous de sa corde entre \\(a\\) et \\(b\\). En-dehors de l'intervalle \\([a,b]\\), elle passe au-dessus de cette corde. Elle doit aussi \u00eatre au-dessus de sa tangente. </p> </li> <li> <p>Propri\u00e9t\u00e9 : la midconvexit\u00e9 - Soit \\(f\\) est une fonction continue sur \\(I\\), alors       </p> </li> </ul>"},{"location":"M/An/Convexit%C3%A9/#epigraphe","title":"\u00c9pigraphe","text":"<ul> <li> <p>D\u00e9finition : Soit \\(f: I \\to \\mathbb{R}\\), on appelle \u00e9pigraphe de \\(f\\) l'ensemble       </p> </li> <li> <p>Soit \\(\\mathcal{C}\\subset \\mathbb{R}^n\\), On dit que \\(\\mathcal{C}\\) est convexe (c'est-\u00e0-dire partie convexe) si       </p> </li> <li> <p>Propri\u00e9t\u00e9 : La fonction \\(f\\) est convexe \\(\\iff\\) L'\u00e9pigraphe \\(\\mathcal{E}_{f}\\) de \\(f\\) est un ensemble convexe</p> </li> </ul>"},{"location":"M/An/Convexit%C3%A9/#proprietes","title":"Propri\u00e9t\u00e9s","text":"<ul> <li> <p>Propri\u00e9t\u00e9 : In\u00e9galit\u00e9 des pentes : On a que \\(f\\) est convexe ssi : pour \\(\\forall (x,y,z) \\in I^3\\), </p> <ul> <li>\u8bb0\u5fc6\uff1a\u56fa\u5b9a\u4e00\u70b9\uff0c\u53e6\u5916\u4e00\u70b9\u5750\u6807\u8d8a\u5927\uff0c\u659c\u7387\u5373\u8d8a\u5927\u3002  </li> </ul> </li> <li> <p>Propri\u00e9t\u00e9 : In\u00e9galit\u00e9 des pentes, Taux d'accroissement d'une fonction convexe : L'application de \\(f\\), \\(f\\) est convexe ssi :       </p> </li> <li> <p>Si \\(f:I\\to \\mathbb{R}\\) une fonction convexe, alors \\(f\\) est continue sur \\(I^\\circ\\).</p> </li> <li> <p>Si la fonction est continue et d\u00e9rivable :       </p> <ul> <li>Soit \\(I\\) un intervalle ouvert de \\(\\mathbb R\\), soit \\(f\\in \\mathscr C(I,\\mathbb R)\\) ayant en tout point une de\u0301rive\u0301e a\u0300 droite telle que \\(f_d'\\) soit croissante, alors \\(f\\) est convexe. Rappel :          Souvent c'est obligatoire d'assurer si \\(\\underset{{x \\to x_0^-}}{\\lim}f_d'(x)&lt;f_d '|_{x=x_0}\\).</li> </ul> </li> <li> <p>Si la fonction est continue et deux fois d\u00e9rivable :       </p> </li> </ul>"},{"location":"M/An/Convexit%C3%A9/#inegalites","title":"In\u00e9galit\u00e9s","text":""},{"location":"M/An/Convexit%C3%A9/#cas-discrete","title":"Cas Discr\u00e8te","text":"<ul> <li> <p>In\u00e9galit\u00e9 de convexit\u00e9 discr\u00e8te : Soit \\(f:I \\to \\mathbb{R}\\) convexe, tq \\((\\lambda_{1},\\dots,\\lambda_{n})\\in(\\mathbb{R}_{+})^n\\), alors           Autrement dit :  L'image du barycentre \u00e0 coefficients positifs est inf\u00e9rieure ou \u00e9gale au barycentre des images.</p> <ul> <li> <p>Cas \\(f: x \\mapsto x^{b/a}\\), \\(\\bar x = x^a\\) :       </p> </li> <li> <p>Cas \\(f = \\exp\\), \\(\\bar x=\\pm\\ln (x)\\) :       </p> </li> </ul> </li> <li> <p>In\u00e9galit\u00e9 de Cauchy-Schwarz discr\u00e8te : Retour vers l'in\u00e9galit\u00e9 de Cauchy-Schwarz :       </p> </li> <li> <p>In\u00e9galit\u00e9 de H\u00f6lder discr\u00e8te : Soit \\(p\\) et \\(q\\) deux nombres r\u00e9els de \\(]1, +\\infty[\\) tels que           Soit \\((a_{1},\\dots,a_{n},b_{1},\\dots,b_{n})\\in \\mathbb{R}_{+}^{2n}\\). Alors,       </p> </li> </ul>"},{"location":"M/An/Convexit%C3%A9/#inegalite-de-jensen","title":"In\u00e9galit\u00e9 de Jensen","text":"<ul> <li> <p>In\u00e9galit\u00e9 de Jensen ou In\u00e9galit\u00e9 de convexit\u00e9 continue : Soit \\(I\\) un intervalle de \\(\\mathbb{R}\\), soit \\(f\\) et \\(h\\) d\u00e9finis, continues par morceaux sur \\(I\\) avec \\(h \\geq 0\\), et telles que       </p> <p>Soit \\(\\phi\\) une fonction convexe d\u00e9finie sur un intervalle ouvert \\(J\\) contenant \\(f(I)\\) et telle que   </p> <p>Alors,   </p> </li> <li> <p>Cas sp\u00e9cial :  </p> </li> <li> <p>In\u00e9galit\u00e9 de Cauchy-Schwarz continue : En justifiant que tous les choses existents, on a :       </p> </li> <li> <p>In\u00e9galit\u00e9 de H\u00f6lder : En justifiant que tous les choses existents, on a :      </p> </li> </ul>"},{"location":"M/An/Topology/","title":"Topology","text":""},{"location":"M/An/Topology/#distance-diametre-norme","title":"Distance, Diam\u00e8tre, Norme","text":"<ul> <li> <p>Soit \\(E\\) un ensemble non vide, on dit qu'une application \\(d\\) d\u00e9finit une distance sur \\(E\\), si \\(d\\) v\u00e9rfie</p> <ul> <li>d\u00e9finie positive :       </li> <li>sym\u00e9trie</li> <li>in\u00e9galit\u00e9 triangulaire :       </li> </ul> </li> <li> <p>Un espace m\u00e9trique not\u00e9 \\((E,d)\\) est un ensemble \\(E\\) muni d'une distance \\(d\\)</p> </li> <li> <p>On appelle diam\u00e8tre de \\(A\\) pour \\((E,d)\\) et \\(A\\) est une partie non vide de \\(E\\) :       </p> </li> <li> <p>Soit \\(E\\) un \\(\\mathbb K\\)-espace vectoriel, on dit que l'application \\(N\\) d\u00e9finit une norme sur \\(E\\), si elle v\u00e9rifie :</p> <ul> <li> <p>d\u00e9finie positive :       </p> </li> <li> <p>homog\u00e9n\u00e9it\u00e9 :      </p> </li> <li>In\u00e9galit\u00e9 triangulaire :       </li> </ul> </li> <li> <p>Si \\((E,N)\\) est un espace vectoriel norm\u00e9, alors l'application d\u00e9finie par :           d\u00e9finit une distance de \\(E\\), cette distance est appel\u00e9e sous-jacente \u00e0 la norme.</p> </li> <li> <p>Exemples des normes :      </p> </li> </ul>"},{"location":"M/An/Topology/#boules-fermees","title":"Boules, Ferm\u00e9es","text":"<p>On soit \\((E,d)\\) un espace m\u00e9trique.</p> <ul> <li> <p>Soit \\(a \u2208 E\\) et \\(r \u2208 R_+\\). On appelle boule ferm\u00e9e de centre \\(a\\) et de rayon \\(r\\) l\u2019ensemble      </p> </li> <li> <p>On appelle boule ouverte de centre \\(a\\) et de rayon \\(r\\) l\u2019ensemble      </p> </li> <li> <p>On appelle sph\u00e8re de centre \\(a\\) et de rayon \\(r\\) l\u2019ensemble      </p> </li> <li> <p>On parle de boule (ou de sph\u00e8re) unit\u00e9 si \\(r=1\\).</p> </li> <li> <p>Soit \\(A\\) est une partie de \\((E,d)\\), on dit que \\(A\\) est born\u00e9e si          ou \\(\\mathrm{diam}(A)&lt; + \\infty\\).</p> </li> <li> <p>Soit \\((x_n)\\) une suite d'\u00e9l\u00e9ments de \\(E\\), \\(\\lambda \\in E\\). On dit que la suite converge vers \\(\\lambda\\) si       </p> <ul> <li>\\(\\lambda\\) est appel\u00e9e limite de la suite.</li> <li>On note :      </li> </ul> </li> <li> <p>Soit \\(F\\subset E\\). On dit \\(F\\) est ferm\u00e9 dans \\(E\\) si      </p> </li> <li> <p>Propri\u00e9t\u00e9s :</p> <ul> <li>\\((F_i)\\) une famille q.q. de ferm\u00e9s de \\(E\\), alors \\(\\bigcap F_i\\) est ferm\u00e9e dans \\(E\\)</li> <li>\\((F_i)\\) une famille finie de ferm\u00e9s de \\(E\\), alors \\(\\bigcup F_i\\) est ferm\u00e9e dans \\(E\\)</li> </ul> </li> <li> <p>Soit \\(A \\subset E\\), on appelle adh\u00e9rence de \\(A\\) dans \\(E\\) le plus petit ferm\u00e9 de \\(E\\) contenant \\(A\\), et on le note \\(\\bar A\\).</p> </li> <li> <p>Soit \\(x\\in E\\),       </p> <ul> <li>L'ensemble \\(\\bar A\\) est donc l'ensemble des limites des suites d'\u00e9l\u00e9ments de \\(A\\).</li> </ul> </li> <li> <p>Soit \\((E,d)\\) un espace m\u00e9trique, alors :      </p> <ul> <li>L'\u00e9galit\u00e9 est pris lorsque \\((E, N)\\) est un espace norm\u00e9e.</li> </ul> </li> </ul>"},{"location":"ML/ML-Basics/","title":"\u6570\u5b66 &amp; \u7406\u8bba\u57fa\u7840","text":""},{"location":"ML/ML-Basics/#linear-regression","title":"Linear Regression \u7ebf\u6027\u56de\u5f52","text":"<ul> <li>\u5b9a\u4e49\u8f93\u51fa\u548c\u5747\u65b9\u8bef\u5dee\u5206\u522b\u4e3a\uff1a\\(\\hat y = w^Tx\\), \\(\\mathrm{MSE} = \\frac{1}{m}\\sum_i(\\hat y - y)_i^2\\)\uff0c\u6211\u4eec\u901a\u8fc7\u6b63\u89c4\u65b9\u7a0b\uff08normal equation\uff09 \u6c42\u5f97\uff1a      </li> </ul>"},{"location":"ML/ML-Basics/#_2","title":"\u6570\u636e\u3001\u5bb9\u91cf","text":"<ul> <li> <p>\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u901a\u8fc7\u6570\u636e\u96c6\u4e0a\u88ab\u79f0\u4e3a\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u6982\u7387\u5206\u5e03\u751f\u6210\uff0c\u6211\u4eec\u505a\u72ec\u7acb\u540c\u5206\u5e03\u5047\u8bbe\uff08i.i.d. assumption\uff09\uff0c\u4ee5\u4f7f\u968f\u673a\u6a21\u578b\u8bad\u7ec3\u8bef\u5dee\u7684\u671f\u671b\u548c\u8be5\u6a21\u578b\u6d4b\u8bd5\u8bef\u5dee\u7684\u671f\u671b\u4e00\u6837\uff1a</p> <ul> <li>\u6837\u672c\u4e92\u76f8\u72ec\u7acb</li> <li>\u8bad\u7ec3\u96c6\u6d4b\u8bd5\u96c6\u540c\u5206\u5e03</li> </ul> </li> <li></li> </ul>"},{"location":"ML/CV/Brain-/","title":"Brain","text":""},{"location":"ML/CV/Brain-/#brain-likeness-brain-score","title":"Brain-likeness: Brain Score","text":"<ol> <li> <p>Brain-Score to evaluate any ANN on how brain-like it is \u2013 focusing on the parts of the brain that have been implicated in visual object recognition.</p> </li> <li> <p>Metrics:</p> <ul> <li>The purpose of neural metrics is to establish how well internal representations of a source system match the internal representations in a target system.</li> </ul> </li> <li> <p>The global Brain-Score as a composite(mean) of neural V4 predictivity score, neural IT predictivity score, and behavioral I2n predictivity score</p> </li> </ol>"},{"location":"ML/CV/Brain-/#references","title":"References","text":"<ol> <li>Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like?     Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J. Majaj, Rishi Rajalingham, Elias B. Issa, Kohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, Kailyn Schmidt, Daniel L. K. Yamins, James J. DiCarlo     bioRxiv 407007; doi: https://doi.org/10.1101/407007</li> </ol>"},{"location":"ML/CV/Brain-/#cornet","title":"CORnet","text":"<p>Keywords: ANN, Feedforwd, Recurrent 1. The relationship between such very deep architectures and the ventral visual pathway is incomplete in at least two ways:</p> <pre><code>- Current state-of-the-art ANNs appear to be **too complex**, compared with the relatively shallow cortical hierarchy.\n\n- Not complex enough in that they **lack recurrent connections** and the resulting neural response dynamics that are commonplace in the ventral visual stream.\n</code></pre> <ol> <li> <p>Therefore, they reduce the model family to its most important elements and then gradually build new ANNs with recurrent and skip connections.</p> </li> <li> <p>COR: core object recognition = the ability to rapidly (&lt;200 ms viewing duration) discriminate a given visual object (e.g., a car, top row) from all other possible visual objects (e.g., bottom row) without any object-specific or location-specific pre-cuing.</p> </li> <li> <p>Model Criteria:</p> <ul> <li>Predictive.</li> </ul> </li> </ol>"},{"location":"ML/CV/Brain-/#references_1","title":"References","text":"<ol> <li>CORnet: Modeling the Neural Mechanisms of Core Object Recognition    Jonas Kubilius, Martin Schrimpf, Aran Nayebi, Daniel Bear, Daniel L. K. Yamins, James J. DiCarlo    bioRxiv 408385; doi: https://doi.org/10.1101/408385</li> </ol>"},{"location":"ML/CV/CNN/","title":"Convolutional Network","text":""},{"location":"ML/CV/CNN/#convolution-operation-and-cross-correlation","title":"Convolution operation and cross-correlation","text":"<ul> <li> <p>Convolution operation: To predict the position of a special object \\(x=x(t)\\), we do an average to the measurements in a period of time. The more recent the measurement is, the more weight it is given. Therefore, we obtain a smooth estimating function \\(s=s(t)\\):       </p> <ul> <li>Only in our example, \\(w\\) is an effective probability density function, and \\(w(x&lt;0)=0\\).</li> <li>\\(x\\) is the input, \\(w\\) is the kernel function, and \\(f\\) is refered to as feature map.</li> </ul> </li> <li> <p>A discrete version of the operation could be written as:       </p> </li> <li> <p>For a two dimensional figure \\(I\\), using a two-dimensional kernel \\(K\\), the equivalent form is:       </p> </li> <li> <p>However, the cross-correlation is more often used in most nets:      </p> <ul> <li>Convolution = Cross-correlation + flipping the kernel<ul> <li>Example of kernel flipping:      </li> </ul> </li> <li>Convolution operations (Math) are commutative, it is because we have flipped the kernel, which refers to the decrease of the index of the kernel while the index of the input is increasing. We ignore the flip as we have abandoned the commutative property.</li> <li>In deep-learning, convolution (DL) refers to the mathematical operation cross-correlation (Math), lacking a flip to the kernel. </li> </ul> </li> </ul>"},{"location":"ML/CV/CNN/#motivations","title":"Motivations","text":"<ul> <li> <p>Sparse Interactions</p> <ul> <li>The size of the kernel \\(\\ll\\) The size of the input, we use small amount of kernels to detect some small features.</li> <li>For a \\(m\\)-dimensional input and a \\(n\\)-dimensional output, a full-connection network have a \\(m\\times n\\) matrix to store the parameters. However, by limiting the number of connections to \\(k&lt;m\\), the matrix is reduced to \\(k \\times n\\).</li> </ul> </li> <li> <p>Parameter sharing</p> <ul> <li>Using same parameters (connected to the kernel elements) to different areas of the input.</li> </ul> </li> <li> <p>Equivariant representations</p> <ul> <li>Equivariance: \\(f(g(x))=g(f(x))\\), for example translation of certain functions. (Rotations or other operations may not be equivariant)</li> </ul> </li> </ul>"},{"location":"ML/CV/CNN/#process","title":"Process","text":"<ul> <li>The first stage, convolutional layer trigger a linear activation response,     </li> <li> <p>The second stage, the detector stage using a non-linear activation function to make the previous output pass through the architecture.</p> </li> <li> <p>The third stage, we use the pooling function to adjust the input.</p> </li> </ul>"},{"location":"ML/CV/CNN/#pooling","title":"Pooling","text":"<ul> <li> <p>The pooling function uses the overall statistical characteristics of the adjacent output at a certain position to replace the output of the network at that location.</p> </li> <li> <p>When the input are slightly translated, the pooling function helps to make the expression of input invariant.</p> </li> <li> <p>We care if a certain feature exists, but the specific position is unimportant.</p> </li> <li> <p>Advantages:</p> </li> <li>Produce a same-sized output for different kinds of input</li> <li>Because the pooling integrates the feedback of all neighbors, it makes it possible that the pooling unit is smaller than the detection unit. We can use the integrated pooling area.</li> </ul>"},{"location":"ML/CV/CNN/#requirements-for-the-task","title":"Requirements for the task","text":"<ul> <li> <p>There is no evident or significant relationship between two locations that are separated by a considerable distance. (Convolution Layer)</p> </li> <li> <p>The task requires focus on a specific location. (Pooling Layer)</p> </li> </ul>"},{"location":"ML/CV/CNN/#improvements-from-convolutional-operations-in-math","title":"Improvements from convolutional operations in math","text":"<ul> <li> <p>Differences:</p> <ul> <li>Multiple convolution processing in the same time: multiple features to be extracted.</li> <li>Input as four dimension: \\((ind,i,j,k)\\)<ul> <li>\\((j,k)\\) coordinates in the picture</li> <li>\\(i\\) the tunnel (RGB)</li> <li>\\(ind\\) the index in the whole batch</li> </ul> </li> <li>Not necessarily invariant</li> </ul> </li> <li> <p>Partially connected networks = Unshared Convolution, cases when we are guaranteed that a certain feature will only appear in a certain part of the picture:      </p> <ul> <li>\\(V_{l,x,y}\\): Input, the value of the position \\((x,y)\\) of the input and \\(l\\) tunnel.</li> <li>\\(W_{i,j,k,l,m,n}\\): The weight matrix (6-dimensional), output \\((i,j,k)\\): \\(i\\) tunnel and position \\((j,k)\\), input \\((l,m,n)\\): \\(l\\) tunnel, \\((m,n)\\) the difference of index</li> <li>\\(Z_{l,x,y}\\): The output, same form with the input</li> </ul> </li> <li> <p>Note: \\(-1\\) comes from the index problem, modern computer usually begins its index from 0.</p> </li> <li> <p>Basic Convolution:      </p> <ul> <li>Kernel: \\(K_{i,j,k,l}\\), \\(i\\) tunnel, connection with the \\(j\\) tunnel of the input, positioned \\((k,l)\\), </li> </ul> </li> <li> <p>Stride: If we want to sample the output at an interval of \\(s\\) pixels in each direction, we can apply a stride of \\(s\\) during the convolution or pooling operation:      </p> </li> </ul>"},{"location":"ML/CV/CNN/#references","title":"References","text":"<ol> <li> <p>A Comprehensive Guide to Convolutional Neural Networks \u2014 the ELI5 way</p> </li> <li> <p>Notes for Deep Learning, by GitHub user windmissing</p> </li> </ol>"},{"location":"P/Ondes%20Ondulatoires/","title":"Ondes Ondulatoires","text":""},{"location":"P/Ondes%20Ondulatoires/#interferences-spectrosocopie","title":"Interf\u00e9rences - Spectrosocopie","text":""},{"location":"P/Ondes%20Ondulatoires/#sources-a-spectre-discret","title":"Sources \u00e0 spectre discret","text":"<ul> <li> <p>Une source \u00e0 spectre discret est consid\u00e9r\u00e9e comme la superpostion d'un ensemble discret de sources parfaitement monochromatiques.</p> </li> <li> <p>Une source monochromatique :      </p> </li> <li> <p>Doublet spectral :</p> <p> </p> <p>Preuve :   </p> <ul> <li> <p>Contrast :      </p> <p>Preuve :  </p> </li> </ul> </li> </ul>"},{"location":"P/Ondes%20Ondulatoires/#sources-reelles-a-spectre-continue","title":"Sources r\u00e9elles \u00e0 spectre continue","text":"<ul> <li> <p>La puissance \u00e9mise par la source dans \\([\\omega , \\omega +\\mathrm{d}\\omega]\\), avec \\(\\mathrm d\\omega &gt;0\\) est :           o\u00f9 \\(B_\\omega\\) est appel\u00e9e la densit\u00e9 spectrale de puissance.</p> <ul> <li>La spectrom\u00e9trie est une science \u00e0 d\u00e9terminer \\(B_\\omega\\).</li> </ul> </li> <li> <p>La fonction \\(B_\\omega(\\omega)\\) est nulle partout sauf dans un intervalle de pulsations centr\u00e9 sur une pulsation central \\(\\omega_0\\), on appelle \\(\\Delta \\omega\\) la largeur spectrale du spectre de la source. </p> </li> <li> <p>On peut \u00e9crire :       </p> </li> <li> <p>L'\u00e9clairement global est la somme des \u00e9clairements de chaque intervalle spectral :       </p> <ul> <li>Contrast :     </li> </ul> </li> </ul>"},{"location":"P/Ondes%20Ondulatoires/#diffraction","title":"Diffraction","text":"<ul> <li>Fonction porte et Int\u00e9grale de Fourier :    </li> </ul> <p>D\u00e9monstration :   </p>"},{"location":"P/Ondes%20Ondulatoires/#diffraction-de-frauhofer-pupilles-fondamentales","title":"Diffraction de Frauhofer, Pupilles Fondamentales","text":"<ul> <li> <p>Configuration de Frauhofer : Source \\(S\\) et observateur \\(M\\) sont \u00e0 l'infini. </p> </li> <li> <p>Cas d'une pupille plane en incidence normale :     </p> </li> </ul> <p>D\u00e9monstration :  </p> <ul> <li>L'amplitude diffract\u00e9e dans le cas d'une incidence quelconque : On remplace \\(\\alpha \\mapsto \\alpha - \\alpha_i,\\; \\beta \\mapsto \\beta - \\beta_i\\) </li> </ul> <p>D\u00e9monstration :  </p> <ul> <li>On place maintenant devant l'ouverture \\((T)\\) un objet de transparence \\(\\tau(P)\\) variable, d\u00e9finie par \\(\\underline{A}(P)= \\tau(P)\\times \\underline{A}(P^-)\\), donc :       </li> </ul>"}]}