{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BrandonLin's Personal Blog","text":"<p>Hi, I'm Brandon Lin, an undergraduate student now studying at Shanghai Jiao Tong University.</p> <p>My github link: Click here!</p>"},{"location":"Cookbooks/Mkdocs/","title":"Mkdocs","text":""},{"location":"Cookbooks/Mkdocs/#installation","title":"Installation","text":"<pre><code>pip install mkdocs\npip install mkdocs-material\npip install https://github.com/mitya57/python-markdown-math/archive/master.zip`\n</code></pre>"},{"location":"Cookbooks/Mkdocs/#create-server-and-publishment","title":"Create, Server and Publishment","text":"<ul> <li> <p>Create a projet: <code>mkdocs new [filename]</code></p> </li> <li> <p>Render to a site: <code>mkdocs serve</code></p> </li> <li> <p>Publish: <code>mkdocs gh-deploy</code></p> </li> </ul>"},{"location":"Cookbooks/Mkdocs/#code","title":"Code","text":"<ul> <li>Code Naming, Code Highlighting, Line Numeration: By adding <code>title=\"name\" linenums=\"1\" hl_lines=\"index\"</code> at the right of the first ```, example : </li> </ul> first.py<pre><code>print(\"Hello World\")\n# Yes, it works!\n</code></pre>"},{"location":"Cookbooks/Mkdocs/#template-for-mkdocsyml","title":"Template for mkdocs.yml","text":"<pre><code>site_name: Test # Change here\nnav:\n- Home: 'index.md'\n- 'Cookbooks': - 'Mkdocs': 'Cookbooks/Mkdocs.md'\ntheme: name: material\nfeatures:\n- naviagation.tabs\n- naviagation.sections\n- toc.integrate\n- navigation.top\n- search.suggest\n- search.highlight\n- content.tabs.link\n- content.code.annotation\n- content.code.copy\nlanguage: en\npalette:\n- scheme: default\ntoggle:\nicon: material/toggle-switch-off-outline\nname: Switch to dark mode\nprimary: teal\naccent: purple\n- scheme: slate\ntoggle:\nicon: material/toggle-switch\nname: Switch to light mode\nprimary: teal\naccent: lime\nextra:\nsocial:\n- icon: fontawesome/brands/github-alt\nlink: https://github.com/Languisher # Change to your github link\ncopyright: | # Change based on your own personal information\n&amp;copy; 2023 &lt;a href=\"https://github.com/Languisher\"  target=\"_blank\" rel=\"noopener\"&gt;Brandon Lin&lt;/a&gt;\nextra_javascript: - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\nmarkdown_extensions:\n- pymdownx.highlight:\nanchor_linenums: true\n- pymdownx.inlinehilite\n- pymdownx.snippets\n- admonition\n- pymdownx.arithmatex:\ngeneric: true\n- footnotes\n- pymdownx.details\n- pymdownx.superfences\n- pymdownx.mark\n- attr_list\n- mdx_math\n</code></pre> <p>Explications:</p> <ul> <li>The <code>nav</code> configuration setting in your mkdocs.yml file defines which pages are included in the global site navigation menu as well as the structure of that menu. If not provided, the navigation will be automatically created by discovering all the Markdown files in the documentation directory.</li> </ul>"},{"location":"Cookbooks/Mkdocs/#references","title":"References","text":"<ul> <li>Basic Tutorial: https://www.mkdocs.org/user-guide/</li> <li> <p>Material Tutorial: https://www.youtube.com/watch?v=Q-YA_dA8C20</p> </li> <li> <p>Mathjax problem: https://stackoverflow.com/questions/27882261/mkdocs-and-mathjax</p> </li> </ul>"},{"location":"M/AL/Endomorphismes/","title":"Endomorphismes","text":"<p>Exercices : TD</p> <p>Notations :</p> <ul> <li> <p>\\(E\\) est un espace vectoriel euclidien de dimension \\(n\\).</p> </li> <li> <p>\\(\\mathscr E = (e_1,\\cdots, e_n)\\), \\(\\mathscr B=(b_1,\\cdots, b_n)\\) deux bases orthonorm\u00e9es de \\(E\\).</p> </li> <li> <p>\\(P_{\\mathscr B}^{\\mathscr E}\\) le matrice de passage d'une matrice muni d'une base \\(\\mathscr E\\) dans celle d'une base \\(\\mathscr B\\).</p> </li> <li> <p>\\(\\mathscr F=(f_1,\\cdots,f_n)\\) une base quelconque de \\(E\\).</p> </li> <li> <p>\\(\\phi:(x,y) \\mapsto &lt;x,y&gt;\\) le produit scalaire de \\(E\\)</p> </li> <li> <p>\\(\\Phi=\\mathrm{Mat}_\\mathscr{F}(\\phi)= [\\phi(f_i,f_j)]_{(i,j)\\in [\\![1,n]\\!]}\\in \\mathrm{S}_n(\\mathbb R)\\) la matrice de \\(\\phi\\) dans la base \\(\\mathscr F\\).</p> </li> </ul> <p>D\u00e9monstration :   </p>"},{"location":"M/AL/Endomorphismes/#resume","title":"R\u00e9sum\u00e9","text":"<ul> <li> \\[   x\\in E \\iff \\begin{cases}\\mathrm{Mat}_{\\mathscr E}(x) = [&lt;e_i,x&gt;]_i\\in \\mathrm{M}_{m,1}\\\\\\mathrm{Mat}_{\\mathscr{B}}(x) = \\;^tP_{\\mathscr E}^\\mathscr B\\cdot \\mathrm{Mat}_\\mathscr E(x)\\end{cases}   \\] </li> <li> \\[   u \\in \\mathscr{L}(E) \\iff \\begin{cases}\\mathrm{Mat}_\\mathscr E(u) = [&lt;e_i,u(e_j)&gt;]_{(i,j)} \\\\ \\mathrm{Mat}_\\mathscr B(u) = \\;^tP_\\mathscr{E}^{\\mathscr{B}}.\\mathrm{Mat}_\\mathscr{E}(u).P_\\mathscr E^\\mathscr B\\\\ \\boxed{\\mathrm{Mat}_\\mathscr E(u^\\star)=\\;^t\\mathrm{Mat}_\\mathscr E(u)}\\end{cases}   \\] </li> <li> \\[   u \\in \\mathscr S(E) \\iff \\begin{cases} \\;^t\\mathrm{Mat}_\\mathscr E(u) = \\mathrm{Mat}_\\mathscr E(u)\\\\\\mathrm{Mat}_\\mathscr E(u) \\in \\mathrm{S}_n(\\mathbb R)\\end{cases}   \\] </li> <li> \\[   u \\in \\mathscr O(E) \\iff \\begin{cases} \\;^t\\mathrm{Mat}_\\mathscr E(u) =( \\mathrm{Mat}_\\mathscr E(u))^{-1}\\\\\\mathrm{Mat}_\\mathscr E(u) \\in \\mathrm{O}_n(\\mathbb R)\\end{cases}   \\] </li> <li> \\[   u \\in \\mathscr A(E) \\iff \\begin{cases} \\;^t\\mathrm{Mat}_\\mathscr E(u) =- \\mathrm{Mat}_\\mathscr E(u)\\\\\\mathrm{Mat}_\\mathscr E(u) \\in \\mathrm{A}_n(\\mathbb R)\\end{cases}   \\] </li> <li> <p>Si on a le choix de la base dans un espace euclidien, on choisit prequement toujours de travailler en base orthonorm\u00e9e !</p> </li> </ul>"},{"location":"M/AL/Endomorphismes/#decompositions","title":"D\u00e9compositions","text":""},{"location":"M/AL/Endomorphismes/#decomposition-de-cholesky","title":"D\u00e9composition de Cholesky","text":"<ul> <li> <p>Si \\(A \\in \\mathrm{S}_n^{++}(\\mathbb R)\\), il existe une unique matrice \\(T\\in \\mathrm{T}_n^+ \\cap \\mathrm{GL}_n(\\mathbb R)\\) \u00e0 coefficients diagonaux \\(&gt;0\\) tel que :     </p> </li> <li> <p>En ce cas, \\(AX = Y \\iff \\begin{cases} \\;^tT.Z = Y\\\\ T.X = Z\\end{cases}\\)</p> </li> </ul> <p>D\u00e9monstration :</p> <ul> <li> <p>Analyse : L'orthomalisation de Gram-Schimidt appliqu\u00e9 \u00e0 \\(\\mathscr{B}=(b_1, \\cdots, b_n)\\) donne \\(\\mathscr{E} = (e_1,\\cdots, e_n)\\). On trouvera       On sait que \\(u \\in \\mathscr{S}^{++}\\), donc \\(\\exists ! v\\in \\mathscr{S}^{++}\\) tel que \\(u=v \\circ v\\), en m\u00eame temps \\(&lt;c_i,u(c_j) = &lt;v(c_i),v(c_j)&gt;\\)</p> </li> <li> <p>Synth\u00e8se :</p> </li> <li> <p>Existence : \\(T = [&lt;e_i,b_j&gt;]\\), o\u00f9 \\((e_1,\\cdots,e_n)\\) orthonorm\u00e9e de \\((b_1, \\cdots,b_n)\\) bien suffit.</p> </li> <li>Unicit\u00e9 : Si \\(A = \\;^tT_1.T_1 = \\;^tT_2.T_2\\), donc \\(\\Delta=T_1.T_2^{-1} = \\;^tT_1^{-1}.^tT_2\\), appartient respectivement \u00e0 \\(\\mathrm{T}^+\\) et \\(\\mathrm{T}^-\\). Dsonc \\(\\Delta\\) est diagonale. Or, \\(\\(A = \\;^tT_1.T_1 = \\;^tT_2.(^t\\Delta.\\Delta).T_2=\\;^tT_2.T_2\\)\\). Cela implique que \\(\\Delta = \\mathrm{id}_E\\).</li> </ul>"},{"location":"M/AL/Exercices-Endomorphismes/","title":"TD - Endomorphismes","text":""},{"location":"M/AL/Exercices-Endomorphismes/#td-8-e1","title":"TD 8 - E1","text":"<p>On consid\u00e8re      </p> <p>On pose pour \\((f,g)\\in E^2\\),      </p> <ol> <li> <p>Montrer rapidement que \\(E\\) est un espace vectoriel ? Que peut-on dire de sa dimension ?</p> </li> <li> <p>Montrer que \\(\\langle\\,,\\,\\rangle\\) est un produit scalaire sur \\(E\\).</p> </li> <li> <p>On consid\u00e8re les trois fonctions     </p> <p>Montrer que ces trois fonctions sont dans \\(E\\), qu'elles sont ind\u00e9pendantes et calculer le volume du parall\u00e9lipip\u00e8de construite \u00e0 partir de ces trois fonctions.</p> </li> </ol>"},{"location":"M/AL/Exercices-Endomorphismes/#correction","title":"Correction","text":"<ol> <li> <p>On va montrer que \\(E\\) est un sev de \\(\\mathscr C(\\mathbb R^+, \\mathbb R)\\)</p> <ul> <li> <p>La fonction nulle est dans \\(E\\), donc \\(E \\ne \\emptyset\\).</p> </li> <li> <p>Soit \\(\\lambda \\in \\mathbb R\\) et soit \\((f,g)\\in E\\), montrons que \\(\\lambda f+g\\in E\\). Soit \\(t\\in \\mathbb R^+\\).    </p> </li> <li> <p>NB : \\(f(t)g(t)e^{-3t}\\) est int\u00e9grable car \\(fg \\le (f+g)/2\\). </p> </li> <li> <p>Tous les parties sont int\u00e9grable, donc \\(\\lambda f+g \\in E\\).</p> </li> <li> <p>Donc \\(E\\) est un sev de \\(\\mathscr C(\\mathbb R^+, \\mathbb R)\\).</p> </li> </ul> </li> <li> <p>Montrons que \\(\\langle\\,,\\,\\rangle\\) est d\u00e9fini positif, sym\u00e9trique et bilin\u00e9aire.</p> <ul> <li>Soit \\(f\\in E\\), alors      </li> <li> <p>De plus, \\(t \\mapsto f^2(t)e^{-3t}\\) est positive et continue, donc \\(\\langle f,f \\rangle=0 \\implies f=0\\).</p> </li> <li> <p>\\(\\langle\\,f,g\\rangle = \\langle\\,g,f\\rangle\\)</p> </li> <li>\\(\\langle\\,f,\\lambda g+h\\rangle = \\lambda\\langle\\,f,g\\rangle +\\langle\\,f,h\\rangle\\)</li> </ul> </li> <li> <p>Calculerons          en utilisant <code>Python</code> :      <pre><code>def scal(f,g):\nt = sp.Symbol(\"t\")\nreturn sp.integrate(f(t)*g(t)*sp.exp(-3*t), (t, 0, +sp.oo))\nf = [sp.exp, sp.sin, sp.cos]\nGram = sp.zeros(3)\nfor i in range(3):\nfor j in range(3):\nGram[i,j] = scal(f[i], f[j])\nGram\n</code></pre>      Calculerons son d\u00e9terminant :       </p> </li> </ol>"},{"location":"M/AL/Exercices-Endomorphismes/#td8-e2","title":"TD8 - E2","text":"<p>Soit \\(E\\) un espace euclidien, \\(\\mathcal{E}\\) un espace affine de direction \\(E\\) et \\(\\phi\\) une application affine de \\(\\mathcal{E}\\) dans \\(\\mathcal{E}\\) dont l'application lin\u00e9aire sous-jacente sera not\u00e9e \\(\\overrightarrow{\\phi}\\).</p> <p>On pose  </p> <ol> <li> <p>Montrer que si \\(E_1=\\{0_{_E}\\}\\), alors \\(\\phi\\) poss\u00e8de un unique point fixe.</p> </li> <li> <p>On suppose que \\(E_1\\ne\\{0_{_E}\\}\\), montrer que \\(\\phi\\) est alors la compos\u00e9e d'une application affine ayant un point fixe et d'une translation (\u00e9ventuellement nulle).</p> </li> <li> <p>On suppose que   Montrer qu'on peut alors choisir la translation parall\u00e8lement \u00e0 \\(E_1\\).</p> </li> </ol>"},{"location":"M/An/Convexit%C3%A9/","title":"Convexit\u00e9","text":""},{"location":"M/An/Convexit%C3%A9/#convexe-concave-affine","title":"Convexe, Concave, Affine","text":"<ul> <li> <p>D\u00e9finition : Soit \\(f:I\\to \\mathbb{R}\\), \\(f\\) est dite convexe si elle v\u00e9rifie       </p> </li> <li> <p>\\(f\\) est dite concave si elle v\u00e9rifie       </p> </li> <li> <p>Si \\(f:I\\to \\mathbb{R}\\) est \u00e0 la fois convexe et concave, alors \\(f\\) est affine.</p> </li> <li> <p>Propri\u00e9t\u00e9 : Sur l'intervalle \\([a,b]\\), la courbe passe en-dessous de sa corde entre \\(a\\) et \\(b\\). En-dehors de l'intervalle \\([a,b]\\), elle passe au-dessus de cette corde.</p> </li> <li> <p>Propri\u00e9t\u00e9 : la midconvexit\u00e9 - Soit \\(f\\) est une fonction continue sur \\(I\\), alors       </p> </li> </ul>"},{"location":"M/An/Convexit%C3%A9/#epigraphe","title":"\u00c9pigraphe","text":"<ul> <li> <p>D\u00e9finition : Soit \\(f: I \\to \\mathbb{R}\\), on appelle \u00e9pigraphe de \\(f\\) l'ensemble       </p> </li> <li> <p>Soit \\(\\mathcal{C}\\subset \\mathbb{R}^n\\), On dit que \\(\\mathcal{C}\\) est convexe (c'est-\u00e0-dire partie convexe) si       </p> </li> <li> <p>Propri\u00e9t\u00e9 : La fonction \\(f\\) est convexe \\(\\iff\\) L'\u00e9pigraphe \\(\\mathcal{E}_{f}\\) de \\(f\\) est un ensemble convexe</p> </li> </ul>"},{"location":"M/An/Convexit%C3%A9/#proprietes","title":"Propri\u00e9t\u00e9s","text":"<ul> <li> <p>Propri\u00e9t\u00e9 : In\u00e9galit\u00e9 des pentes : On a que \\(f\\) est convexe ssi : pour \\(\\forall (x,y,z) \\in I^3\\),       </p> </li> <li> <p>Propri\u00e9t\u00e9 : In\u00e9galit\u00e9 des pentes, Taux d'accroissement d'une fonction convexe : L'application de \\(f\\), \\(f\\) est convexe ssi :       </p> </li> <li> <p>Si \\(f:I\\to \\mathbb{R}\\) une fonction convexe, alors \\(f\\) est continue sur \\(I^\\circ\\).</p> </li> <li> <p>Si la fonction est continue et d\u00e9rivable :       </p> </li> <li> <p>Si la fonction est continue et deux fois d\u00e9rivable :       </p> </li> </ul>"},{"location":"M/An/Topology/","title":"Topology","text":""},{"location":"M/An/Topology/#distance-diametre-norme","title":"Distance, Diam\u00e8tre, Norme","text":"<ul> <li> <p>Soit \\(E\\) un ensemble non vide, on dit qu'une application \\(d\\) d\u00e9finit une distance sur \\(E\\), si \\(d\\) v\u00e9rfie</p> <ul> <li>d\u00e9finie positive :       </li> <li>sym\u00e9trie</li> <li>in\u00e9galit\u00e9 triangulaire :       </li> </ul> </li> <li> <p>Un espace m\u00e9trique not\u00e9 \\((E,d)\\) est un ensemble \\(E\\) muni d'une distance \\(d\\)</p> </li> <li> <p>On appelle diam\u00e8tre de \\(A\\) pour \\((E,d)\\) et \\(A\\) est une partie non vide de \\(E\\) :       </p> </li> <li> <p>Soit \\(E\\) un \\(\\mathbb K\\)-espace vectoriel, on dit que l'application \\(N\\) d\u00e9finit une norme sur \\(E\\), si elle v\u00e9rifie :</p> <ul> <li>d\u00e9finie positive</li> <li>homog\u00e9n\u00e9it\u00e9 :      </li> <li>In\u00e9galit\u00e9 triangulaire :       </li> </ul> </li> </ul>"},{"location":"M/An/Topology/#boules-ferme","title":"Boules, Ferm\u00e9","text":"<ul> <li> <p>Soit \\((E,d)\\) un espace m\u00e9trique, \\(a \u2208 E\\) et \\(r \u2208 R_+\\). On appelle boule ferm\u00e9e de centre \\(a\\) et de rayon \\(r\\) l\u2019ensemble      </p> </li> <li> <p>On appelle boule ouverte de centre \\(a\\) et de rayon \\(r\\) l\u2019ensemble      </p> </li> <li> <p>On appelle sph\u00e8re de centre \\(a\\) et de rayon \\(r\\) l\u2019ensemble      </p> </li> <li> <p>On parle de boule (ou de sph\u00e8re) unit\u00e9 si \\(r=1\\).</p> </li> <li> <p>Soit \\((E,d)\\) un espace me\u0301trique et \\(F\\) une partie de \\(E\\). Alors \\(F\\) est ferm\u00e9 dans \\(E\\) si, et seulement si       </p> </li> <li> <p>Soit \\((E,d)\\) un espace me\u0301trique et \\(A \\subset E\\), on appelle adh\u00e9rence de \\(A\\) dans \\(E\\) le plus petit ferm\u00e9 de \\(E\\) contenant \\(A\\), et on le note \\(\\bar A\\).</p> </li> <li> <p>Soit \\(x\\in E\\),       </p> </li> </ul>"},{"location":"ML/CNN/","title":"Convolutional Network","text":""},{"location":"ML/CNN/#convolution-operation-and-cross-correlation","title":"Convolution operation and cross-correlation","text":"<ul> <li> <p>Convolution operation: To predict the position of a special object \\(x=x(t)\\), we do an average to the measurements in a period of time. The more recent the measurement is, the more weight it is given. Therefore, we obtain a smooth estimating function \\(s=s(t)\\):       </p> <ul> <li>Only in our example, \\(w\\) is an effective probability density function, and \\(w(x&lt;0)=0\\).</li> <li>\\(x\\) is the input, \\(w\\) is the kernel function, and \\(f\\) is refered to as feature map.</li> </ul> </li> <li> <p>A discrete version of the operation could be written as:       </p> </li> <li> <p>For a two dimensional figure \\(I\\), using a two-dimensional kernel \\(K\\), the equivalent form is:       </p> </li> <li> <p>However, the cross-correlation is more often used in most nets:      </p> <ul> <li>Convolution = Cross-correlation + flipping the kernel<ul> <li>Example of kernel flipping:      </li> </ul> </li> <li>Convolution operations (Math) are commutative, it is because we have flipped the kernel, which refers to the decrease of the index of the kernel while the index of the input is increasing. We ignore the flip as we have abandoned the commutative property.</li> <li>In deep-learning, convolution (DL) refers to the mathematical operation cross-correlation (Math), lacking a flip to the kernel. </li> </ul> </li> </ul>"},{"location":"ML/CNN/#motivations","title":"Motivations","text":"<ul> <li> <p>Sparse Interactions</p> <ul> <li>The size of the kernel \\(\\ll\\) The size of the input, we use small amount of kernels to detect some small features.</li> <li>For a \\(m\\)-dimensional input and a \\(n\\)-dimensional output, a full-connection network have a \\(m\\times n\\) matrix to store the parameters. However, by limiting the number of connections to \\(k&lt;m\\), the matrix is reduced to \\(k \\times n\\).</li> </ul> </li> <li> <p>Parameter sharing</p> <ul> <li>Using same parameters (connected to the kernel elements) to different areas of the input.</li> </ul> </li> <li> <p>Equivariant representations</p> <ul> <li>Equivariance: \\(f(g(x))=g(f(x))\\), for example translation of certain functions. (Rotations or other operations may not be equivariant)</li> </ul> </li> </ul>"},{"location":"ML/CNN/#process","title":"Process","text":"<ul> <li>The first stage, convolutional layer trigger a linear activation response,     </li> <li> <p>The second stage, the detector stage using a non-linear activation function to make the previous output pass through the architecture.</p> </li> <li> <p>The third stage, we use the pooling function to adjust the input.</p> </li> </ul>"},{"location":"ML/CNN/#pooling","title":"Pooling","text":"<ul> <li> <p>The pooling function uses the overall statistical characteristics of the adjacent output at a certain position to replace the output of the network at that location.</p> </li> <li> <p>When the input are slightly translated, the pooling function helps to make the expression of input invariant.</p> </li> <li> <p>We care if a certain feature exists, but the specific position is unimportant.</p> </li> <li> <p>Advantages:</p> </li> <li>Produce a same-sized output for different kinds of input</li> <li>Because the pooling integrates the feedback of all neighbors, it makes it possible that the pooling unit is smaller than the detection unit. We can use the integrated pooling area.</li> </ul>"},{"location":"ML/CNN/#requirements-for-the-task","title":"Requirements for the task","text":"<ul> <li> <p>There is no evident or significant relationship between two locations that are separated by a considerable distance. (Convolution Layer)</p> </li> <li> <p>The task requires focus on a specific location. (Pooling Layer)</p> </li> </ul>"},{"location":"ML/CNN/#improvements-from-convolutional-operations-in-math","title":"Improvements from convolutional operations in math","text":"<ul> <li> <p>Differences:</p> <ul> <li>Multiple convolution processing in the same time: multiple features to be extracted.</li> <li>Input as four dimension: \\((ind,i,j,k)\\)<ul> <li>\\((j,k)\\) coordinates in the picture</li> <li>\\(i\\) the tunnel (RGB)</li> <li>\\(ind\\) the index in the whole batch</li> </ul> </li> <li>Not necessarily invariant</li> </ul> </li> <li> <p>Partially connected networks = Unshared Convolution, cases when we are guaranteed that a certain feature will only appear in a certain part of the picture:      </p> <ul> <li>\\(V_{l,x,y}\\): Input, the value of the position \\((x,y)\\) of the input and \\(l\\) tunnel.</li> <li>\\(W_{i,j,k,l,m,n}\\): The weight matrix (6-dimensional), output \\((i,j,k)\\): \\(i\\) tunnel and position \\((j,k)\\), input \\((l,m,n)\\): \\(l\\) tunnel, \\((m,n)\\) the difference of index</li> <li>\\(Z_{l,x,y}\\): The output, same form with the input</li> </ul> </li> <li> <p>Note: \\(-1\\) comes from the index problem, modern computer usually begins its index from 0.</p> </li> <li> <p>Basic Convolution:      </p> <ul> <li>Kernel: \\(K_{i,j,k,l}\\), \\(i\\) tunnel, connection with the \\(j\\) tunnel of the input, positioned \\((k,l)\\), </li> </ul> </li> <li> <p>Stride: If we want to sample the output at an interval of \\(s\\) pixels in each direction, we can apply a stride of \\(s\\) during the convolution or pooling operation:      </p> </li> </ul>"},{"location":"ML/CNN/#references","title":"References","text":"<ol> <li> <p>A Comprehensive Guide to Convolutional Neural Networks \u2014 the ELI5 way</p> </li> <li> <p>Notes for Deep Learning, by GitHub user windmissing</p> </li> </ol>"},{"location":"ML/ML-Basics/","title":"\u6570\u5b66 &amp; \u7406\u8bba\u57fa\u7840","text":""},{"location":"ML/ML-Basics/#linear-regression","title":"Linear Regression \u7ebf\u6027\u56de\u5f52","text":"<ul> <li>\u5b9a\u4e49\u8f93\u51fa\u548c\u5747\u65b9\u8bef\u5dee\u5206\u522b\u4e3a\uff1a\\(\\hat y = w^Tx\\), \\(\\mathrm{MSE} = \\frac{1}{m}\\sum_i(\\hat y - y)_i^2\\)\uff0c\u6211\u4eec\u901a\u8fc7\u6b63\u89c4\u65b9\u7a0b\uff08normal equation\uff09 \u6c42\u5f97\uff1a      </li> </ul>"},{"location":"ML/ML-Basics/#_2","title":"\u6570\u636e\u3001\u5bb9\u91cf","text":"<ul> <li> <p>\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u901a\u8fc7\u6570\u636e\u96c6\u4e0a\u88ab\u79f0\u4e3a\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u6982\u7387\u5206\u5e03\u751f\u6210\uff0c\u6211\u4eec\u505a\u72ec\u7acb\u540c\u5206\u5e03\u5047\u8bbe\uff08i.i.d. assumption\uff09\uff0c\u4ee5\u4f7f\u968f\u673a\u6a21\u578b\u8bad\u7ec3\u8bef\u5dee\u7684\u671f\u671b\u548c\u8be5\u6a21\u578b\u6d4b\u8bd5\u8bef\u5dee\u7684\u671f\u671b\u4e00\u6837\uff1a</p> <ul> <li>\u6837\u672c\u4e92\u76f8\u72ec\u7acb</li> <li>\u8bad\u7ec3\u96c6\u6d4b\u8bd5\u96c6\u540c\u5206\u5e03</li> </ul> </li> <li></li> </ul>"},{"location":"P/Ondes%20Ondulatoires/","title":"Ondes Ondulatoires","text":""},{"location":"P/Ondes%20Ondulatoires/#interferences-spectrosocopie","title":"Interf\u00e9rences - Spectrosocopie","text":""},{"location":"P/Ondes%20Ondulatoires/#sources-a-spectre-discret","title":"Sources \u00e0 spectre discret","text":"<ul> <li> <p>Une source \u00e0 spectre discret est consid\u00e9r\u00e9e comme la superpostion d'un ensemble discret de sources parfaitement monochromatiques.</p> </li> <li> <p>Une source monochromatique :      </p> </li> <li> <p>Doublet spectral :</p> <p> </p> <p>Preuve :   </p> <ul> <li> <p>Contrast :      </p> <p>Preuve :  </p> </li> </ul> </li> </ul>"},{"location":"P/Ondes%20Ondulatoires/#sources-reelles-a-spectre-continue","title":"Sources r\u00e9elles \u00e0 spectre continue","text":"<ul> <li> <p>La puissance \u00e9mise par la source dans \\([\\omega , \\omega +\\mathrm{d}\\omega]\\), avec \\(\\mathrm d\\omega &gt;0\\) est :           o\u00f9 \\(B_\\omega\\) est appel\u00e9e la densit\u00e9 spectrale de puissance.</p> <ul> <li>La spectrom\u00e9trie est une science \u00e0 d\u00e9terminer \\(B_\\omega\\).</li> </ul> </li> <li> <p>La fonction \\(B_\\omega(\\omega)\\) est nulle partout sauf dans un intervalle de pulsations centr\u00e9 sur une pulsation central \\(\\omega_0\\), on appelle \\(\\Delta \\omega\\) la largeur spectrale du spectre de la source. </p> </li> <li> <p>On peut \u00e9crire :       </p> </li> <li> <p>L'\u00e9clairement global est la somme des \u00e9clairements de chaque intervalle spectral :       </p> <ul> <li>Contrast :     </li> </ul> </li> </ul>"},{"location":"P/Ondes%20Ondulatoires/#diffraction","title":"Diffraction","text":"<ul> <li>Fonction porte et Int\u00e9grale de Fourier :    </li> </ul> <p>D\u00e9monstration :   </p>"},{"location":"P/Ondes%20Ondulatoires/#diffraction-de-frauhofer-pupilles-fondamentales","title":"Diffraction de Frauhofer, Pupilles Fondamentales","text":"<ul> <li> <p>Configuration de Frauhofer : Source \\(S\\) et observateur \\(M\\) sont \u00e0 l'infini. </p> </li> <li> <p>Cas d'une pupille plane en incidence normale :     </p> </li> </ul> <p>D\u00e9monstration :  </p> <ul> <li>L'amplitude diffract\u00e9e dans le cas d'une incidence quelconque : On remplace \\(\\alpha \\mapsto \\alpha - \\alpha_i,\\; \\beta \\mapsto \\beta - \\beta_i\\) </li> </ul> <p>D\u00e9monstration :  </p> <ul> <li>On place maintenant devant l'ouverture \\((T)\\) un objet de transparence \\(\\tau(P)\\) variable, d\u00e9finie par \\(\\underline{A}(P)= \\tau(P)\\times \\underline{A}(P^-)\\), donc :       </li> </ul>"}]}